{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Feature Extraction and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's workshop will address various concepts in the Natural Language Processing pipeline aimed at feature and information extraction for subsequent use in machine learning tasks. A fundmental understanding of Python is necessary. We will cover:\n",
    "\n",
    "1. Pre-processing\n",
    "2. Preparing and declaring your own corpus\n",
    "3. POS-Tagging\n",
    "4. Dependency Parsing\n",
    "5. NER\n",
    "6. Sentiment Analysis\n",
    "7. Classification\n",
    "\n",
    "\n",
    "You will need:\n",
    "\n",
    "* NLTK ( `$ pip install nltk`)\n",
    "* the parser wrapper requires the [Stanford Parser](http://nlp.stanford.edu/software/lex-parser.shtml#Download) (in Java)\n",
    "* the NER wrapper requires the [Stanford NER](http://nlp.stanford.edu/software/CRF-NER.shtml#Download) (in Java)\n",
    "* scikit-learn ( `$ pip install scikit-learn`)\n",
    "* keras ( `$ pip install keras`)\n",
    "* gensim ( `$ pip install gensim`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This won't be covered much today, but regex and basic python string methods are most important in preprocessing tasks. NLTK does, however, offer an array of tokenizers and stemmers for various languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The term-document model\n",
    "\n",
    "This is also sometimes referred to as \"bag-of-words\" by those who don't think very highly of it. The term document model looks at language as individual communicative efforts that contain one or more tokens. The kind and number of the tokens in a document tells you something about what is attempting to be communicated, and the order of those tokens is ignored.\n",
    "\n",
    "This is the primary method still used for most text analysis, although models utilizing word embeddings are beginning to take hold. We will discuss word embeddings briefly at the end.\n",
    "\n",
    "To start with, let's import NLTK and load a document from their toy corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Regex Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('webtext')\n",
    "document = nltk.corpus.webtext.open('grail.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(document[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "snippet = document.split(\"\\n\")[8]\n",
    "print(snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(r'coconuts', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with `str.find`, we can search for plain text. But `re` also gives us the option for searching for patterns of bytes - like only alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(r'[a-z]', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we've told re to search for the first sequence of bytes that is only composed of lowercase letters between `a` and `z`. We could get the letters at the end of each sentence by including a bang at the end of the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(r'[a-z]!', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things happening here:\n",
    "\n",
    "1. `[` and `]` do not mean 'bracket'; they are special characters which mean 'anything of this class'\n",
    "2. we've only matched one letter each\n",
    "\n",
    "Re is flexible about how you specify numbers - you can match none, some, a range, or all repetitions of a sequence or character class.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`{x}`     | exactly x repetitions\n",
    "`{x,y}`   | between x and y repetitions\n",
    "`?`       | 0 or 1 repetition\n",
    "`*`       | 0 or many repetitions\n",
    "`+`       | 1 or many repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the power of regular expressions are their special characters. Common ones that you'll see are:\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`.`       | match anything except a newline\n",
    "`^`       | match the start of a line\n",
    "`$`       | match the end of a line\n",
    "`\\s`      | matches any whitespace or newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to grab all of Arthur's speech without grabbing the name `ARTHUR` itself?\n",
    "\n",
    "If we wanted to do this using base string manipulation, we would need to do something like:\n",
    "\n",
    "```\n",
    "split the document into lines\n",
    "create a new list of just lines that start with ARTHUR\n",
    "create a newer list with ARTHUR removed from the front of each element\n",
    "```\n",
    "\n",
    "Regex gives us a way of doing this in one line, by using something called groups. Groups are pieces of a pattern that can be ignored, negated, or given names for later retrieval.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`(x)`     | match x\n",
    "`(?:x)`   | match x but don't capture it\n",
    "`(?P<x>)` | match something and give it name x\n",
    "`(?=x)`   | match only if string is followed by x\n",
    "`(?!x)`   | match only if string is not followed by x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.findall(r'(?:ARTHUR: )(.+)', document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because we are using `findall`, the regex engine is capturing and returning the normal groups, but not the non-capturing group. For complicated, multi-piece regular expressions, you may need to pull groups out separately. You can do this with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = re.compile(r'(?P<name>[A-Z ]+)(?::)(?P<line>.+)')\n",
    "match = re.search(p, document)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(match.group('name'))\n",
    "print(match.group('line'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the regex patter `p` above to print the `set` of unique characters in *Monty Python*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = re.findall(p, document)\n",
    "chars = set([x[0] for x in matches])\n",
    "print(chars, len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have 84 different characters.\n",
    "\n",
    "Now use the `set` you made above to gather all dialogue into a character `dictionary`, with the keys being the character name and the value being a list of dialogues.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_dict = {}\n",
    "for n in chars:\n",
    "    char_dict[n] = re.findall(re.compile(r'(?:' + n + ': )(.+)'), document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_dict[\"PATSY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = '''Hello, my name is Chris. \n",
    "I'll be talking about the python library NLTK today. \n",
    "NLTK is a popular tool to conduct text processing tasks in NLP.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"Notice the difference!\")\n",
    "print()\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print()\n",
    "print(\"vs.\")\n",
    "print()\n",
    "\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also tokenize sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of sentences with a list of tokenized words is generally the accepted format for most libraries for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "print(snowball.stem('running'))\n",
    "print(snowball.stem('eats'))\n",
    "print(snowball.stem('embarassed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But watch out for errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(snowball.stem('cylinder'))\n",
    "print(snowball.stem('cylindrical'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or collision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(snowball.stem('vacation'))\n",
    "print(snowball.stem('vacate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why lemmatizing, if the computing power and time is sufficient, is always preferable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "print(wordnet.lemmatize('vacation'))\n",
    "print(wordnet.lemmatize('vacate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Declaring a corpus in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can use NLTK on strings and lists of sentences, it's better to formally declare your corpus, as this will take care of the above for you and provide methods to access them. For our purposes today, we'll use a corpus of [book summaries](http://www.cs.cmu.edu/~dbamman/booksummaries.html). I've changed them into a folder of .txt files for demonstration. The file below will convert the .tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = \"texts/\"  # relative path to texts.\n",
    "my_texts = PlaintextCorpusReader(corpus_root, '.*txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a text corpus, on which we can run all the basic preprocessing methods. To list all the files in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_texts.fileids()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_texts.words('To Kill A Mockingbird.txt')  # uses punkt tokenizer like above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_texts.sents('To Kill A Mockingbird.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also add as paragraph method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_texts.paras('To Kill A Mockingbird.txt')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these to a variable to look at the next step on a low level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_sents = my_texts.sents('To Kill A Mockingbird.txt')\n",
    "print (m_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a corpus, or text, from which we can get any of the statistics you learned in Day 3 of the Python workshop. We will review some of these functions once we get some more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) POS-Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many situations, in which \"tagging\" words (or really anything) may be useful in order to determine or calculate trends, or for further text analysis to extract meaning. NLTK contains several methods to achieve this, from simple regex to more advanced machine learning models models.\n",
    "\n",
    "It is important to note that in Natural Language Processing (NLP), POS (Part of Speech) tagging is the most common use for tagging, but the actual tag can be anything. Other applications include sentiment analysis and NER (Named Entity Recognition). Tagging is simply labeling a word to a specific category via a tuple.\n",
    "\n",
    "Nevertheless, for training more advanced tagging models, POS tagging is nearly essential. If you are defining a machine learning model to predict patterns in your text, these patterns will most likley rely on, among other things, POS features. You will therefore first tag POS and then use the POS as a feature in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On a low-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging is creating a tuple of (word, tag) for every word in a text or corpus. For example: \"My name is Chris\" may be tagged for POS as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My/PossessivePronoun name/Noun is/Verb Chris/ProperNoun ./Period\n",
    "\n",
    "*NB: type 'nltk.data.path' to find the path on your computer to your downloaded nltk corpora. You can explore these files to see how large corpora are formatted.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice how the text is annotated, using a forward slash to match the word to its tag. So how can we get this to a useful form for Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import str2tuple\n",
    "\n",
    "line = \"My/Possessive_Pronoun name/Noun is/Verb Chris/Proper_Noun ./Period\"\n",
    "tagged_sent = [str2tuple(t) for t in line.split()]\n",
    "\n",
    "print (tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further analysis of tags with NLTK requires a *list* of sentences, otherwise you will get an index error on higher level methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, these tags are a bit verbose, the standard tagging conventions follow the Penn Treebank (more in a second): https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's stock English `pos_tag` tagger is a perceptron tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "m_tagged_sent = pos_tag(m_sents[0])\n",
    "print (m_tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these tags mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import help\n",
    "help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_tagged_all = [pos_tag(sent) for sent in m_sents]\n",
    "print(m_tagged_all[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find and aggregate certain parts of speech too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "def find_tags(tag_prefix, tagged_text):\n",
    "    cfd = ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    return dict((tag, cfd[tag].most_common(5)) for tag in cfd.conditions()) #cfd.conditions() yields all tags possibilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_tagged_words = [item for sublist in m_tagged_all for item in sublist]\n",
    "\n",
    "tagdict = find_tags('JJ', m_tagged_words)\n",
    "for tag in sorted(tagdict):\n",
    "    print(tag, tagdict[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin to quantify syntax by look at environments of words, so what commonly follows a verb?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "tags = [b[1] for (a, b) in nltk.bigrams(m_tagged_words) if a[1].startswith('VB')]\n",
    "fd1 = nltk.FreqDist(tags)\n",
    "\n",
    "print (\"To Kill A Mockingbird\")\n",
    "fd1.tabulate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tagged corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how tagging works, we can quickly tag all of our documents, but we'll only do a few hundred from the much larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sents = {}\n",
    "for fid in my_texts.fileids()[::10]:\n",
    "    tagged_sents[fid.split(\".\")[0]] = [pos_tag(sent) for sent in my_texts.sents(fid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sents[\"Harry Potter and the Prisoner of Azkaban\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute frequencies are available through NLTK's `FreqDist` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tags = []\n",
    "all_tups = []\n",
    "\n",
    "for k in tagged_sents.keys():\n",
    "    for s in tagged_sents[k]:\n",
    "        for t in s:\n",
    "            all_tags.append(t[1])\n",
    "            all_tups.append(t)\n",
    "\n",
    "nltk.FreqDist(all_tags).tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = ['NN', 'VB', 'JJ']\n",
    "for t in tags:\n",
    "    tagdict = find_tags(t, all_tups)\n",
    "    for tag in sorted(tagdict):\n",
    "        print(tag, tagdict[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this to other genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "for c in brown.categories():\n",
    "    tagged_words = brown.tagged_words(categories=c)  # not universal tagset\n",
    "    tag_fd = nltk.FreqDist(tag for (word, tag) in tagged_words)\n",
    "    print(c.upper())\n",
    "    tag_fd.tabulate(10)\n",
    "    print()\n",
    "    tags = ['NN', 'VB', 'JJ']\n",
    "    for t in tags:\n",
    "        tagdict = find_tags(t, tagged_words)\n",
    "        for tag in sorted(tagdict):\n",
    "            print(tag, tagdict[tag])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at what linguistic environment words are in on a low level, below lists all the words preceding \"love\" in the romance category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "brown_news_text = brown.words(categories='romance')\n",
    "sorted(set(a for (a, b) in nltk.bigrams(brown_news_text) if b == 'love'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While tagging parts of speech can be helpful for certain NLP tasks, dependency parsing is better at extracting real relationships within a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar = \"/Users/chench/Documents/stanford-parser-full-2015-12-09/stanford-parser.jar\",\n",
    "                                             path_to_models_jar = \"/Users/chench/Documents/stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models.jar\")\n",
    "\n",
    "result = dependency_parser.raw_parse_sents(['I shot an elephant in my sleep.', 'It was great.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the program takes longer to run, I will not run it on the entire corpus, but an example is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for r in result:\n",
    "    for o in r:\n",
    "        trips = list(o.triples())  # ((head word, head tag), rel, (dep word, dep tag))\n",
    "        for t in trips:\n",
    "            print(t)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokening, tagging, and parser, one of the last steps in the pipeline is NER. Identifying named entities can be useful in determing many different relationships, and often serves as a prerequisite to mapping textual relationships within a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "ner_tag = StanfordNERTagger(\n",
    "        '/Users/chench/Documents/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "        '/Users/chench/Documents/stanford-ner-2015-12-09/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyprind\n",
    "\n",
    "ner_sents = {}\n",
    "books = [\"To Kill A Mockingbird.txt\", \"Harry Potter and the Prisoner of Azkaban.txt\"]\n",
    "\n",
    "for fid in books:\n",
    "    bar = pyprind.ProgBar(len(my_texts.sents(fid)), monitor=True, bar_char=\"#\")\n",
    "    tagged_sents = []\n",
    "    for sent in my_texts.sents(fid):\n",
    "        tagged_sents.append(ner_tag.tag(sent))\n",
    "        bar.update()\n",
    "    ner_sents[fid.split(\".\")[0]] = tagged_sents\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look on the low level at a single summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ner_sents[\"To Kill A Mockingbird\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ner_sents[\"Harry Potter and the Prisoner of Azkaban\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from nltk import FreqDist\n",
    "\n",
    "NER = {\"LOCATION\": [],\n",
    "       \"PERSON\": [],\n",
    "       \"ORGANIZATION\": [],\n",
    "       }\n",
    "\n",
    "for sentence in ner_sents[\"To Kill A Mockingbird\"]:\n",
    "    for tag, chunk in groupby(sentence, lambda x: x[1]):\n",
    "        if tag != \"O\":\n",
    "            NER[tag].append(\" \".join(w for w, t in chunk))\n",
    "\n",
    "if NER[\"LOCATION\"]:\n",
    "    print(\"Locations:\")\n",
    "    FreqDist(NER[\"LOCATION\"]).tabulate()\n",
    "    print()\n",
    "\n",
    "if NER[\"PERSON\"]:\n",
    "    print(\"Persons:\")\n",
    "    FreqDist(NER[\"PERSON\"]).tabulate()\n",
    "    print()\n",
    "\n",
    "if NER[\"ORGANIZATION\"]:\n",
    "    print(\"Organizations\")\n",
    "    FreqDist(NER[\"ORGANIZATION\"]).tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NER = {\"LOCATION\": [],\n",
    "       \"PERSON\": [],\n",
    "       \"ORGANIZATION\": [],\n",
    "       }\n",
    "\n",
    "for k in ner_sents.keys():\n",
    "    for sentence in ner_sents[k]:\n",
    "        for tag, chunk in groupby(sentence, lambda x: x[1]):\n",
    "            if tag != \"O\":\n",
    "                NER[tag].append(\" \".join(w for w, t in chunk))\n",
    "\n",
    "if NER[\"LOCATION\"]:\n",
    "    print(\"Locations:\")\n",
    "    FreqDist(NER[\"LOCATION\"]).tabulate()\n",
    "    print()\n",
    "\n",
    "if NER[\"PERSON\"]:\n",
    "    print(\"Persons:\")\n",
    "    FreqDist(NER[\"PERSON\"]).tabulate()\n",
    "    print()\n",
    "\n",
    "if NER[\"ORGANIZATION\"]:\n",
    "    FreqDist(NER[\"ORGANIZATION\"]).tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While earlier sentiment analysis was based on simple dictionary look-up methods denoting words as positive or negative, or assigning numerical values to words, newer methods are better able to take a word's or sentence's environment into account. VADER (Valence Aware Dictionary and sEntiment Reasoner) is one such example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(sid.polarity_scores(\"I really don't like that book.\")[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fid in books:\n",
    "    print(fid.upper())\n",
    "    sent_pols = [sid.polarity_scores(s)[\"compound\"] for s in sent_tokenize(my_texts.raw(fid))]\n",
    "    for i, s in enumerate(my_texts.sents(fid)):\n",
    "        print(s, sent_pols[i])\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "    print(\"Mean: \", np.mean(sent_pols))\n",
    "    print()\n",
    "    print(\"=\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the IMDB [movie review database](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics, tree, cross_validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RandomizedLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "data = {\"train\": {\"pos\": [], \"neg\": []},\n",
    "        \"test\": {\"pos\": [], \"neg\": []}}\n",
    "\n",
    "txt_types = [(\"train\", \"neg\"), (\"train\", \"pos\"), (\"test\", \"neg\"), (\"test\", \"pos\")]\n",
    "\n",
    "for t in txt_types:\n",
    "    for txt_file in glob.glob(\"data/\" + t[0] + \"/\" + t[1] + \"/*.txt\"):\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            text = f.read()\n",
    "        data[t[0]][t[1]].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(data[\"train\"][\"pos\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(data[\"train\"][\"neg\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get training + test data\n",
    "import numpy as np\n",
    "\n",
    "X_train = data[\"train\"][\"pos\"] + data[\"train\"][\"neg\"]\n",
    "y_train = np.append(np.ones(len(data[\"train\"][\"pos\"])), np.zeros(len(data[\"train\"][\"neg\"])))\n",
    "\n",
    "X_test = data[\"test\"][\"pos\"] + data[\"test\"][\"neg\"]\n",
    "y_test = np.append(np.ones(len(data[\"test\"][\"pos\"])), np.zeros(len(data[\"test\"][\"neg\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *tfidf*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a pipeline - SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', OneVsRestClassifier(LinearSVC(random_state=0)))\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit using pipeline\n",
    "clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "predicted = clf.predict(X_test)\n",
    "clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print metrics\n",
    "print(metrics.classification_report(y_test, predicted)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = cross_validation.cross_val_score(text_clf, X_train + X_test, np.append(y_train, y_test), cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_imdb_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks and word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are the first successful attempt to move away from the \"bag of words\" model of language. Instead of looking at word frequencies, and vocabulary usage, word embeddings aim to retain syntactic information. Generally, a neural network model *will not* remove stopwords or punctuation, because they are vital to the model itself.\n",
    "\n",
    "Embedding first changes a tokenized sentence into a vector of numbers, with each unique token being its own number.\n",
    "\n",
    "e.g.:\n",
    "\n",
    "~~~\n",
    "[[\"I\", \"like\", \"coffee\", \".\"], [\"I\", \"like\", \"my\", \"coffee\", \"without\", \"sugar\", \".\"]]\n",
    "~~~\n",
    "\n",
    "is tranformed to:\n",
    "\n",
    "~~~\n",
    "[[43, 75, 435, 98], [43, 75, 10, 435, 31, 217, 98]]\n",
    "~~~\n",
    "\n",
    "Notice, the \"I\"s, the \"likes\", the \"coffees\", and the \".\"s, all have the same assignment.\n",
    "\n",
    "The model is created by taking these numbers, and creating a high dimensional vector by mapping every word to its surrounding, creating a sort of \"cloud\" of words, where words used in a similar syntactic, and often semantic, fashion, will cluster closer together.\n",
    "\n",
    "One of the drawbacks of word2vec is the volume of data necessary for a decent analysis.\n",
    "\n",
    "Let's see how to code this into a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to one-hot encode the text, but let's limit the features to the most common 20,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_features = 20000\n",
    "all_words = []\n",
    "\n",
    "for text in X_train + X_test:\n",
    "    all_words.extend(text.split())\n",
    "unique_words_ordered = [x[0] for x in Counter(all_words).most_common()]\n",
    "word_ids = {}\n",
    "rev_word_ids = {}\n",
    "for i, x in enumerate(unique_words_ordered[:max_features-1]):\n",
    "    word_ids[x] = i + 1  # so we can pad with 0s\n",
    "    rev_word_ids[i + 1] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_one_hot = []\n",
    "for text in X_train:\n",
    "    t_ids = [word_ids[x] for x in text.split() if x in word_ids]\n",
    "    X_train_one_hot.append(t_ids)\n",
    "    \n",
    "X_test_one_hot = []\n",
    "for text in X_test:\n",
    "    t_ids = [word_ids[x] for x in text.split() if x in word_ids]\n",
    "    X_test_one_hot.append(t_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Keras, a popular Theano wrapper, to quickly build an NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train_one_hot, maxlen=maxlen)  # so arrays are uniform\n",
    "X_test = sequence.pad_sequences(X_test_one_hot, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
