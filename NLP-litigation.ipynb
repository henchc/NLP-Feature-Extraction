{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in file (text format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"litigation.txt\", \"r\") as f:\n",
    "    raw_text_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data is not yet disambiguated, the first step is to always cut down what you're working with into separate documents. To figure out how to best do this, try to find distinct patterns in the transitions between cases. When you're reading through it as a human, what tells you that this is the beginning of a new case? In data wrangling there is no one right answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse out separate cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two features stuck me immediately: 1) The header is always centered, 2) it ends with a date, maybe more than one, but at least one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to first write a regex to match these dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "date_re = re.compile(r\"(January|February|March|April|May|June|July|August|September|October|November|December)\\s[1-3]?[0-9]+,\\s[0-9]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I iterate through the lines of the file searching for centered text. To check that this is a header, I look to make sure it begins in capital letters. The end of the header is marked with the date. I use the `binary` variable to turn on and off my collection of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to collect strings\n",
    "all_headers = []\n",
    "body_indices = []\n",
    "\n",
    "# to form sub strings\n",
    "header = \"\"\n",
    "binary = 0\n",
    "start = 0\n",
    "for i, l in enumerate(raw_text_lines):\n",
    "    if l.startswith(\"        \"):\n",
    "        if binary == 0:\n",
    "            if l.strip()[:2].isupper() and l.strip()[:2].isalpha():\n",
    "                binary = 1\n",
    "                body_indices.append((start, i))\n",
    "\n",
    "        if i < len(raw_text_lines):\n",
    "            if re.search(date_re, l) and re.search(date_re, raw_text_lines[i+1]) == None:\n",
    "                header += l\n",
    "                all_headers.append(header)\n",
    "                header = \"\"\n",
    "                binary = 0\n",
    "                start = i + 1\n",
    "            \n",
    "        if binary == 1:\n",
    "            header += l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pair back the headers with the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head_body = []\n",
    "\n",
    "for i, pair in enumerate(body_indices[1:]):\n",
    "    head_body.append((all_headers[i], ''.join(raw_text_lines[pair[0]:pair[1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(head_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(head_body[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(head_body[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest method to extract entities is to use [Stanford's NER](http://nlp.stanford.edu/software/CRF-NER.shtml#Download) tagger, with the NLTK wrapper. As we are working with phrases and not necessarily sentences, the accuracy will not be as high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import word_tokenize\n",
    "\n",
    "ner_tag = StanfordNERTagger(\n",
    "        '/Users/chench/Documents/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "        '/Users/chench/Documents/stanford-ner-2015-12-09/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "NER = {\"LOCATION\": [],\n",
    "       \"ORGANIZATION\": [],\n",
    "       \"PERSON\": []}\n",
    "for l in head_body[1][0].split(\"\\n\"):\n",
    "    NER_line = ner_tag.tag(word_tokenize(l))\n",
    "    for tag, chunk in groupby(NER_line, lambda x: x[1]):\n",
    "            if tag != \"O\":\n",
    "                NER_word = \" \".join(w for w, t in chunk)  # join consecutive chunks\n",
    "                NER[tag].append(NER_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(NER[\"ORGANIZATION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time and Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datetime` library is incredibly useful to get more quantitative data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parse all of the headers and find the dates buried in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "for l in head_body[0][0].split(\"\\n\"):\n",
    "    try:\n",
    "        date = parser.parse(l, fuzzy=True)\n",
    "        date_string = date.strftime(\"%Y-%m-%d\")\n",
    "        if date_string != datetime.datetime.now().strftime(\"%Y-%m-%d\") and 1900 < date.year < 2020:\n",
    "            print(l)\n",
    "            print(date)\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to get the difference in days between cases \"argued\" and \"decided\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diff_arg_dec(header):\n",
    "    argued = None\n",
    "    decided = None\n",
    "    for l in header.split('\\n'):\n",
    "        if \"Argued\" in l:\n",
    "            try:\n",
    "                argued = parser.parse(l, fuzzy=True)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif \"Decided\" in l:\n",
    "            try:\n",
    "                decided = parser.parse(l, fuzzy=True)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    if argued and decided:\n",
    "        days_diff = (decided-argued).days\n",
    "        if days_diff > 0:\n",
    "            return days_diff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arg_dec_diffs = []\n",
    "for h in head_body:\n",
    "    diff = get_diff_arg_dec(h[0])\n",
    "    if diff:\n",
    "        arg_dec_diffs.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(len(arg_dec_diffs), np.mean(arg_dec_diffs), np.median(arg_dec_diffs), np.std(arg_dec_diffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about \"filed\" and \"decided\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diff_fil_dec(header):\n",
    "    decided = None\n",
    "    filed = None\n",
    "    for l in header.split('\\n'):\n",
    "        if \"Decided\" in l:\n",
    "            try:\n",
    "                decided = parser.parse(l, fuzzy=True)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif \"Filed\" in l:\n",
    "            try:\n",
    "                filed = parser.parse(l, fuzzy=True)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    if decided and filed:\n",
    "        days_diff = (filed-decided).days\n",
    "        if days_diff > 0:\n",
    "            return days_diff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fil_dec_diffs = []\n",
    "for h in head_body:\n",
    "    diff = get_diff_fil_dec(h[0])\n",
    "    if diff:\n",
    "        fil_dec_diffs.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(fil_dec_diffs), np.mean(fil_dec_diffs), np.median(fil_dec_diffs), np.std(fil_dec_diffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all cases have a \"decided\" line, let's see how productive the courts are being:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decided = []\n",
    "for h in head_body:\n",
    "    for l in h[0].split('\\n'):\n",
    "        if \"Decided\" in l:\n",
    "            try:\n",
    "                d_date = parser.parse(l, fuzzy=True)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    decided.append(d_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(decided), len(set(decided)), len(head_body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When were the most cases decided?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "pop_dates = Counter(decided).most_common()\n",
    "pop_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [x[0] for x in pop_dates]\n",
    "y = [x[1] for x in pop_dates]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(x, y, width=10)\n",
    "ax.xaxis_date()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent co-referencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some text has something like the following: U.S. Patent No. 5,301,105 ('105 patent).\n",
    "\n",
    "How can we find all of these pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "pattern = re.compile(r\"Patent No\\. (?P<number>[0-9\\,]+)\\s\\((?:the\\s)?'(?P<alt>[0-9]+) patent\\)\")\n",
    "for x in head_body:\n",
    "    pairs.extend(re.findall(pattern, x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(pairs))\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_dict = {int(k.replace(\",\", \"\")): int(v.replace(\",\", \"\")) for (k, v) in pairs}\n",
    "print(id_dict[5337753])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract just the outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "pos_outcomes = []\n",
    "for i in head_body[:10]:\n",
    "    body = i[1]\n",
    "    outcome = body[body.find(\"OUTCOME:\"):].split('\\n\\n')[0]\n",
    "    pos_outcome = pos_tag(word_tokenize(outcome)[2:])\n",
    "    pos_outcomes.append(pos_outcome)\n",
    "    print(pos_outcome)\n",
    "    print()\n",
    "    \n",
    "for o in pos_outcomes:\n",
    "    for w in o:\n",
    "        if w[1].startswith(\"VB\"):\n",
    "            print(w[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the `X` and `y` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for i in head_body:\n",
    "    body = i[1]\n",
    "    outcome = body[body.find(\"OUTCOME:\"):].split('\\n\\n')[0]\n",
    "    if \"revers\" in outcome:\n",
    "        X.append(body)\n",
    "        y.append(1)\n",
    "        \n",
    "    elif \"affirm\" in outcome:\n",
    "        X.append(body)\n",
    "        y.append(0)\n",
    "\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the necessary `scikit-learn` code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the TFIDF for train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X)\n",
    "X_train = tfidf.transform(X_train)\n",
    "X_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and get score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_class = LinearSVC()\n",
    "model = svc_class.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing only in a `scikit-learn` pipeline, and k-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LinearSVC(random_state=0))\n",
    "                     ])\n",
    "scores = cross_validation.cross_val_score(text_clf, X, y, cv=2)\n",
    "print(scores, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract useful features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "top10 = np.argsort(model.coef_[0])[-10:]\n",
    "print(list(feature_names[j] for j in top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on using text features in models, see the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html) for feature extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
